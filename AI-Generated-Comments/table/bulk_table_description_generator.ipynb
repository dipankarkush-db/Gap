{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25ce2374-27ef-486f-bb3e-9f146b13beae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Bulk table description generator\n",
    "\n",
    "\n",
    "Customization required\n",
    "- Configure LLM prompt as required\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35769a3c-6669-4e9e-8d5f-c396d24c3f1a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Set up parameters"
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"Catalog\", \"\", \"Enter Catalog Name (Mandatory):\")\n",
    "dbutils.widgets.text(\"Schema\", \"\", \"Enter Schema Name (Optional):\")\n",
    "dbutils.widgets.text(\"Table\", \"\", \"Enter Table Name (Optional):\")\n",
    "dbutils.widgets.text(\"Output Path\", \"\", \"Enter Output Path (Mandatory):\")\n",
    "dbutils.widgets.text(\"Model Serving Endpoint Name\", \"\", \"Model Serving Endpoint Name (Mandatory):\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93091953-14e7-409b-a00d-9e4b1d2ac918",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog = dbutils.widgets.get(\"Catalog\")\n",
    "schema = dbutils.widgets.get(\"Schema\")\n",
    "table = dbutils.widgets.get(\"Table\")\n",
    "output_path = dbutils.widgets.get(\"Output Path\")\n",
    "endpoint_name = dbutils.widgets.get(\"Model Serving Endpoint Name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6fe5004-f9bf-47b2-a6fe-57b6bb6fbc34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"{catalog},{schema},{table},{output_path},{endpoint_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d46230e7-abe7-4786-8fa6-c78bc05c76d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "def get_table_columns(catalog: str, schema: str = None, table: str = None):\n",
    "    \"\"\"\n",
    "    Get all columns and datatypes for one or more tables in Databricks,\n",
    "    returning a dict with keys (catalog, schema, table) and values as\n",
    "    comma-separated \"col_name col_type\" strings.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    try:\n",
    "        # Case 1: Specific catalog/schema/table provided\n",
    "        if catalog and schema and table:\n",
    "            df = spark.table(f\"{catalog}.{schema}.{table}\")\n",
    "            schema_fields = [f\"{f.name} {f.dataType.simpleString()}\" for f in df.schema.fields]\n",
    "            results[(catalog, schema, table)] = \", \".join(schema_fields)\n",
    "            return results\n",
    "\n",
    "        # Case 2: Need to loop through all catalogs/schemas/tables\n",
    "        catalogs = [catalog] if catalog else [row.catalog_name for row in spark.sql(\"SHOW CATALOGS\").collect()]\n",
    "\n",
    "        for cat in catalogs:\n",
    "            schemas = [schema] if schema else [row.databaseName for row in spark.sql(f\"SHOW SCHEMAS IN {cat}\").collect()]\n",
    "            \n",
    "            for sch in schemas:\n",
    "                try:\n",
    "                    tables = [table] if table else [row.tableName for row in spark.sql(f\"SHOW TABLES IN {cat}.{sch}\").collect()]\n",
    "                except AnalysisException:\n",
    "                    # Skip schema if not accessible\n",
    "                    continue\n",
    "\n",
    "                for tbl in tables:\n",
    "                    try:\n",
    "                        df = spark.table(f\"{cat}.{sch}.{tbl}\")\n",
    "                        schema_fields = [f\"{f.name} {f.dataType.simpleString()}\" for f in df.schema.fields]\n",
    "                        results[(cat, sch, tbl)] = \", \".join(schema_fields)\n",
    "                    except AnalysisException:\n",
    "                        # Skip table if not accessible\n",
    "                        continue\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error while fetching columns: {e}\")\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1247b656-5a1c-4fca-bf17-a56c4b21a993",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Function to retrieve the table column comments for a given catalog, schema, table.\n",
    "def get_table_comments(catalog, table_column_details, schema=None, table=None):\n",
    "    \"\"\"\n",
    "    Generate AI-assisted comments for tables, dynamically including schema details\n",
    "    from the table_column_details dict for each row.\n",
    "    \"\"\"\n",
    "    if table_column_details is None:\n",
    "        raise ValueError(\"table_column_details dict must be provided\")\n",
    "\n",
    "    # Convert the dict into a DataFrame for joining\n",
    "    rows = [\n",
    "        (cat, sch, tbl, col_str)\n",
    "        for (cat, sch, tbl), col_str in table_column_details.items()\n",
    "    ]\n",
    "    schema_str_df = spark.createDataFrame(rows, [\"catalog\", \"schema\", \"table\", \"schema_str\"])\n",
    "    schema_str_df.createOrReplaceTempView(\"table_columns_view\")\n",
    "\n",
    "    # Build the query with a LEFT JOIN to dynamically get schema_str per row\n",
    "    query = f\"\"\"\n",
    "        SELECT \n",
    "            t.table_catalog,\n",
    "            t.table_schema,\n",
    "            t.table_name,\n",
    "            t.comment IS NULL OR length(t.comment) == 0 AS replace_comment,\n",
    "            t.comment AS existing_comment,\n",
    "            ai_query(\n",
    "                '{endpoint_name}',\n",
    "                'Generate a paragraph of description of the type of information that the table \"' ||\n",
    "                    t.table_name ||\n",
    "                    '\" in schema \"' ||\n",
    "                    t.table_schema ||\n",
    "                    '\" within the catalog \"' ||\n",
    "                    t.table_catalog ||\n",
    "                    '\" would contain (based on the name and datatypes of the columns: ' ||\n",
    "                    COALESCE(c.schema_str, '') ||\n",
    "                    '). This will be used as a table description, so there is no need to mention that this is a table within a schema within a catalog.'\n",
    "            ) AS new_comment\n",
    "        FROM system.information_schema.tables AS t\n",
    "        LEFT JOIN table_columns_view AS c\n",
    "          ON t.table_catalog = c.catalog\n",
    "         AND t.table_schema = c.schema\n",
    "         AND t.table_name = c.table\n",
    "        WHERE t.table_catalog = :catalog\n",
    "    \"\"\"\n",
    "\n",
    "    if schema:\n",
    "        query += \" AND t.table_schema = :schema\"\n",
    "    if table:\n",
    "        query += \" AND t.table_name = :table\"\n",
    "\n",
    "    query += \" ORDER BY t.table_catalog, t.table_schema, t.table_name\"\n",
    "\n",
    "    table_comments = spark.sql(query, args={\"catalog\": catalog, \"schema\": schema, \"table\": table})\n",
    "    return table_comments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee293167-070b-4b00-a4b2-a648a6052e5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "table_column_details=get_table_columns(catalog, schema, table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a4db910-c8d8-4743-805e-ddb0a45e28b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "commented_tables = get_table_comments(catalog, table_column_details, schema, table)\n",
    "display(commented_tables)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c44d954a-54ba-43a3-810e-1aca3731e0fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### In case user wants to update some table comments after reviewing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d882dd35-e5d2-4981-8688-8009a8c7eda8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Example mapping of updates you want to apply\n",
    "# updates = [\n",
    "#     (catalog, \"fgac\", \"customer\", \"This is the updated comment for customer.\"),\n",
    "#     (catalog, \"fgac\", \"customer_pii_data_parquet\", \"This is the updated comment for customer_pii_data_parquet.\"),\n",
    "# ]\n",
    "\n",
    "updates = []\n",
    "\n",
    "if not updates:\n",
    "  commented_tables_updated=commented_tables\n",
    "else:\n",
    "  # Create a DataFrame with the updates\n",
    "  updates_df = spark.createDataFrame(updates, [\"table_catalog\", \"table_schema\", \"table_name\", \"updated_comment\"])\n",
    "\n",
    "  # Left join with the original DataFrame\n",
    "  commented_tables_updated = (\n",
    "      commented_tables\n",
    "      .join(\n",
    "          updates_df,\n",
    "          on=[\"table_catalog\", \"table_schema\", \"table_name\"],\n",
    "          how=\"left\"\n",
    "      )\n",
    "      .withColumn(\n",
    "          \"new_comment\",\n",
    "          F.when(F.col(\"updated_comment\").isNotNull(), F.col(\"updated_comment\"))\n",
    "          .otherwise(F.col(\"new_comment\"))\n",
    "      )\n",
    "      .drop(\"updated_comment\")  # cleanup temp column\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0f76963-f143-46d5-b3e2-ec7151fcdd0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(commented_tables_updated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "389694c0-a8e3-47d3-a023-b5f7663edfb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Choose your desired file format\n",
    "# commented_columns.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(output_path + \"/csv\")\n",
    "commented_tables_updated.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").json(output_path + \"/json\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5261002272468012,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2,
    "widgetLayout": [
     {
      "breakBefore": false,
      "name": "Catalog",
      "width": 260
     },
     {
      "breakBefore": false,
      "name": "Schema",
      "width": 260
     },
     {
      "breakBefore": false,
      "name": "Table",
      "width": 260
     },
     {
      "breakBefore": false,
      "name": "Output Path",
      "width": 260
     },
     {
      "breakBefore": false,
      "name": "Overwrite Columns",
      "width": 168
     },
     {
      "breakBefore": false,
      "name": "Generating Prompt",
      "width": 352
     },
     {
      "breakBefore": false,
      "name": "Additional Information",
      "width": 352
     }
    ]
   },
   "notebookName": "bulk_table_description_generator",
   "widgets": {
    "Catalog": {
     "currentValue": "dkushari_uc",
     "nuid": "b9885deb-d306-4dda-a77f-027aebfd37c1",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Enter Catalog Name (Mandatory):",
      "name": "Catalog",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Enter Catalog Name (Mandatory):",
      "name": "Catalog",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "Model Serving Endpoint Name": {
     "currentValue": "databricks-claude-3-7-sonnet",
     "nuid": "6eed4a12-655a-43a4-bdbc-ac1fcde754d9",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Model Serving Endpoint Name (Mandatory):",
      "name": "Model Serving Endpoint Name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Model Serving Endpoint Name (Mandatory):",
      "name": "Model Serving Endpoint Name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "Output Path": {
     "currentValue": "/Volumes/dkushari_uc/fgac/bulk_comments/tables",
     "nuid": "9dbccc67-425b-41e4-ac3a-98f5d7db16ce",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Enter Output Path (Mandatory):",
      "name": "Output Path",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Enter Output Path (Mandatory):",
      "name": "Output Path",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "Schema": {
     "currentValue": "fgac",
     "nuid": "77e0720b-a517-4eec-ac80-09cfe3562767",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Enter Schema Name (Optional):",
      "name": "Schema",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Enter Schema Name (Optional):",
      "name": "Schema",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "Table": {
     "currentValue": "",
     "nuid": "9fc3c05f-d8d4-4e4e-8e1a-95268e66c373",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Enter Table Name (Optional):",
      "name": "Table",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Enter Table Name (Optional):",
      "name": "Table",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
