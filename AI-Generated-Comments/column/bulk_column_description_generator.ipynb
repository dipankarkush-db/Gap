{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25ce2374-27ef-486f-bb3e-9f146b13beae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Bulk table column description generator\n",
    "\n",
    "\n",
    "Customization required\n",
    "- Configure LLM prompt as required\n",
    "\n",
    "Authors\n",
    "- Scott Eade\n",
    "- Sierra Yap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35769a3c-6669-4e9e-8d5f-c396d24c3f1a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Set up parameters"
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"Catalog\", \"\", \"Enter Catalog Name (Mandatory):\")\n",
    "dbutils.widgets.text(\"Schema\", \"\", \"Enter Schema Name (Optional):\")\n",
    "dbutils.widgets.text(\"Table\", \"\", \"Enter Table Name (Optional):\")\n",
    "dbutils.widgets.text(\"Output Path\", \"\", \"Enter Output Path (Mandatory):\")\n",
    "dbutils.widgets.text(\"Model Serving Endpoint Name\", \"databricks-meta-llama-3-3-70b-instruct\", \"Model Serving Endpoint Name (Mandatory):\")\n",
    "dbutils.widgets.text(\"Sample Data Limit\", \"5\", \"Sample Data Limit (Mandatory):\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93091953-14e7-409b-a00d-9e4b1d2ac918",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog = dbutils.widgets.get(\"Catalog\")\n",
    "schema = dbutils.widgets.get(\"Schema\")\n",
    "table = dbutils.widgets.get(\"Table\")\n",
    "output_path = dbutils.widgets.get(\"Output Path\")\n",
    "endpoint_name = dbutils.widgets.get(\"Model Serving Endpoint Name\")\n",
    "data_limit = int(dbutils.widgets.get(\"Sample Data Limit\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6fe5004-f9bf-47b2-a6fe-57b6bb6fbc34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"{catalog},{schema},{table},{output_path},{endpoint_name},{data_limit}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f8af218-1b17-4763-92f9-a1b0bffda010",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Row\n",
    "\n",
    "def get_sample_data(catalog, limit, schema=\"\", table=\"\"):\n",
    "    \"\"\"\n",
    "    Fetch up to `limit` rows from each table and return dict keyed by (catalog, schema, table).\n",
    "    \"\"\"\n",
    "    sample_data = {}\n",
    "\n",
    "    # Build list of tables\n",
    "    query = f\"\"\"\n",
    "        SELECT table_catalog, table_schema, table_name\n",
    "        FROM system.information_schema.tables\n",
    "        WHERE table_catalog = :catalog\n",
    "    \"\"\"\n",
    "    if schema:\n",
    "        query += \" AND table_schema = :schema\"\n",
    "        if table:\n",
    "            query += \" AND table_name = :table\"\n",
    "\n",
    "    tables = spark.sql(query, args={\"catalog\": catalog, \"schema\": schema, \"table\": table}).collect()\n",
    "\n",
    "    for t in tables:\n",
    "        full_table_name = f\"{t.table_catalog}.{t.table_schema}.{t.table_name}\"\n",
    "        data_query = f\"SELECT * FROM {full_table_name} LIMIT {limit}\"\n",
    "\n",
    "        try:\n",
    "            rows = spark.sql(data_query).toPandas().to_dict(orient=\"records\")\n",
    "            sample_data[(t.table_catalog, t.table_schema, t.table_name)] = rows\n",
    "        except Exception as e:\n",
    "            sample_data[(t.table_catalog, t.table_schema, t.table_name)] = [{\"error\": str(e)}]\n",
    "\n",
    "    return sample_data\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c52d3f7d-2613-48e1-a6d4-fa315ca7f739",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "get_sample_data(catalog, data_limit, schema, table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43c21adc-bcb9-4b96-b728-7246c1f41b3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "def get_column_comments(catalog, limit, schema=\"\", table=\"\"):\n",
    "    \"\"\"\n",
    "    Build AI prompts for each column including schema info + sample rows,\n",
    "    then call ai_query for column description generation.\n",
    "    Returns: PySpark DataFrame\n",
    "    \"\"\"\n",
    "    # Step 1: get sample rows\n",
    "    samples = get_sample_data(catalog, limit, schema, table)\n",
    "\n",
    "    # Step 2: get column metadata\n",
    "    query = f\"\"\"\n",
    "      SELECT c.table_catalog, c.table_schema, c.table_name, c.column_name, c.data_type,\n",
    "             c.ordinal_position,\n",
    "             c.comment IS NULL or length(c.comment) == 0 AS replace_comment,\n",
    "             c.comment AS existing_comment\n",
    "      FROM system.information_schema.columns AS c\n",
    "      JOIN system.information_schema.tables AS t \n",
    "           USING (table_catalog, table_schema, table_name)\n",
    "      WHERE c.table_catalog = :catalog\n",
    "    \"\"\"\n",
    "    if schema:\n",
    "        query += \" AND c.table_schema = :schema\"\n",
    "        if table:\n",
    "            query += \" AND c.table_name = :table\"\n",
    "    query += \" ORDER BY c.table_catalog, c.table_schema, c.table_name, c.ordinal_position\"\n",
    "\n",
    "    columns = spark.sql(query, args={\"catalog\": catalog, \"schema\": schema, \"table\": table}).collect()\n",
    "\n",
    "    # Step 3: Build prompt + call ai_query\n",
    "    rows_out = []\n",
    "    for col in columns:\n",
    "        key = (col.table_catalog, col.table_schema, col.table_name)\n",
    "        sample_rows = samples.get(key, [])\n",
    "        sample_text = str(sample_rows[:3])  # include only first 3 rows\n",
    "\n",
    "        prompt = (\n",
    "            f'Generate a one-sentence description of the type of information in column \"{col.column_name}\" '\n",
    "            f'from table \"{col.table_name}\" in schema \"{col.table_schema}\" within catalog \"{col.table_catalog}\". '\n",
    "            f'The column data type is \"{col.data_type}\". '\n",
    "            f'Here are some sample rows from the table: {sample_text}. '\n",
    "            f'This will be used as a column description, so avoid mentioning schema or catalog.'\n",
    "        )\n",
    "        print(prompt)\n",
    "        # Call ai_query dynamically\n",
    "        ai_sql = f\"SELECT ai_query('{endpoint_name}', :prompt) AS new_comment\"\n",
    "        new_comment = spark.sql(ai_sql, args={\"prompt\": prompt}).collect()[0].new_comment\n",
    "\n",
    "        rows_out.append(Row(\n",
    "            table_catalog=col.table_catalog,\n",
    "            table_schema=col.table_schema,\n",
    "            table_name=col.table_name,\n",
    "            column_name=col.column_name,\n",
    "            ordinal_position=col.ordinal_position,\n",
    "            existing_comment=col.existing_comment,\n",
    "            replace_comment=col.replace_comment,\n",
    "            new_comment=new_comment\n",
    "        ))\n",
    "    \n",
    "    schema_out = StructType([\n",
    "    StructField(\"table_catalog\", StringType(), True),\n",
    "    StructField(\"table_schema\", StringType(), True),\n",
    "    StructField(\"table_name\", StringType(), True),\n",
    "    StructField(\"column_name\", StringType(), True),\n",
    "    StructField(\"ordinal_position\", StringType(), True),\n",
    "    StructField(\"existing_comment\", StringType(), True),\n",
    "    StructField(\"replace_comment\", StringType(), True),\n",
    "    StructField(\"new_comment\", StringType(), True)\n",
    "])\n",
    "    # Step 4: Convert list of Rows -> PySpark DataFrame\n",
    "    df_out = spark.createDataFrame(\n",
    "    rows_out,\n",
    "    schema=schema_out\n",
    ")\n",
    "\n",
    "    return df_out\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a4db910-c8d8-4743-805e-ddb0a45e28b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "commented_columns = get_column_comments(catalog, data_limit, schema, table)\n",
    "display(commented_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48d63a20-4655-4625-8f06-bb88ae55af48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### In case user wants to update some column comments after reviewing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71d18efe-e6d0-4053-91f9-868f49c71d47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Example mapping of updates you want to apply\n",
    "# updates = [\n",
    "#     (catalog, \"fgac\", \"cust\", \"rec_id\", \"This is the new updated comment for cust's record_id.\", True),\n",
    "#     (catalog, \"fgac\", \"cust\", \"ssn\", \"This is the new updated comment for SSN in cust.\", True),\n",
    "# ]\n",
    "\n",
    "updates = []\n",
    "\n",
    "if not updates:\n",
    "    commented_columns_updated = commented_columns\n",
    "else:\n",
    "    # Create a DataFrame with the updates\n",
    "    updates_df = spark.createDataFrame(\n",
    "        updates,\n",
    "        [\"table_catalog\", \"table_schema\", \"table_name\", \"column_name\", \"updated_comment\", \"replace_comment_update\"]\n",
    "    )\n",
    "\n",
    "    # Left join with the original DataFrame\n",
    "    commented_columns_updated = (\n",
    "        commented_columns\n",
    "        .join(\n",
    "            updates_df,\n",
    "            on=[\"table_catalog\", \"table_schema\", \"table_name\", \"column_name\"],\n",
    "            how=\"left\"\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"new_comment\",\n",
    "            F.when(F.col(\"updated_comment\").isNotNull(), F.col(\"updated_comment\"))\n",
    "             .otherwise(F.col(\"new_comment\"))\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"replace_comment\",\n",
    "            F.when(F.col(\"replace_comment_update\").isNotNull(), F.col(\"replace_comment_update\")) # use update value if provided\n",
    "             .otherwise(F.col(\"replace_comment\"))  # fallback to existing\n",
    "        )\n",
    "        .drop(\"updated_comment\", \"replace_comment_update\")  # cleanup temp columns\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56c1c983-52b5-4802-a945-d4a6c9605270",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1756407760556}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(commented_columns_updated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "389694c0-a8e3-47d3-a023-b5f7663edfb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Choose your desired file format\n",
    "# commented_columns.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(output_path + \"/csv\")\n",
    "commented_columns_updated.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").json(output_path + \"/json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5927ea3-3d73-4bec-b313-499d835a8722",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d3c7487-ad1b-44a8-bd33-c3c4f093644b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "baa4794d-5bc4-4c77-925b-2474f91f5e17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7529c72c-9799-4339-85c3-9fd08fe4980c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1247b656-5a1c-4fca-bf17-a56c4b21a993",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Function to retrieve the table column comments for a given catalog, schema, table.\n",
    "# def get_column_comments(catalog, schema=\"\", table=\"\"):\n",
    "#   query = f\"\"\"\n",
    "#     SELECT c.table_catalog, c.table_schema, c.table_name, c.column_name, c.ordinal_position, c.comment IS NULL or length(c.comment) == 0 AS replace_comment, c.comment AS existing_comment\n",
    "#     , ai_query('{endpoint_name}', 'Generate a 1 sentence description of the type of information that the column \"' || c.column_name || '\" from the table \"' || c.table_name || '\" in schema \"' || c.table_schema || '\" within the catalog \"' || c.table_catalog || '\" would contain (the data type of the column is \"' || c.data_type || '\"). This will be used as a column description, so there is no need to mention that this is a column within a schema within a catalog.') AS new_comment\n",
    "#     FROM system.information_schema.columns AS c\n",
    "#     JOIN system.information_schema.tables AS t USING (table_catalog, table_schema, table_name)\n",
    "#     WHERE table_catalog = :catalog\n",
    "#     \"\"\"\n",
    "#   if schema:\n",
    "#     query += \" AND table_schema = :schema\"\n",
    "#     if table:\n",
    "#       query += \" AND table_name = :table\"\n",
    "#   query += \" ORDER BY table_catalog, table_schema, table_name, ordinal_position\"\n",
    "#   # query += \" LIMIT 5\"\n",
    "#   column_comments = spark.sql(query, args = {\"catalog\": catalog, \"schema\": schema, \"table\": table})\n",
    "#   return column_comments"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5261002272465370,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2,
    "widgetLayout": [
     {
      "breakBefore": false,
      "name": "Catalog",
      "width": 260
     },
     {
      "breakBefore": false,
      "name": "Schema",
      "width": 260
     },
     {
      "breakBefore": false,
      "name": "Table",
      "width": 260
     },
     {
      "breakBefore": false,
      "name": "Output Path",
      "width": 260
     },
     {
      "breakBefore": false,
      "name": "Overwrite Columns",
      "width": 168
     },
     {
      "breakBefore": false,
      "name": "Generating Prompt",
      "width": 352
     },
     {
      "breakBefore": false,
      "name": "Additional Information",
      "width": 352
     }
    ]
   },
   "notebookName": "bulk_column_description_generator",
   "widgets": {
    "Catalog": {
     "currentValue": "dkushari_uc",
     "nuid": "b9885deb-d306-4dda-a77f-027aebfd37c1",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Enter Catalog Name (Mandatory):",
      "name": "Catalog",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Enter Catalog Name (Mandatory):",
      "name": "Catalog",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "Model Serving Endpoint Name": {
     "currentValue": "databricks-meta-llama-3-3-70b-instruct",
     "nuid": "d6f422ef-c22d-4567-a3f6-82be668674cc",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "databricks-meta-llama-3-3-70b-instruct",
      "label": "Model Serving Endpoint Name (Mandatory):",
      "name": "Model Serving Endpoint Name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "databricks-meta-llama-3-3-70b-instruct",
      "label": "Model Serving Endpoint Name (Mandatory):",
      "name": "Model Serving Endpoint Name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "Output Path": {
     "currentValue": "/Volumes/dkushari_uc/fgac/bulk_comments/columns",
     "nuid": "9dbccc67-425b-41e4-ac3a-98f5d7db16ce",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Enter Output Path (Mandatory):",
      "name": "Output Path",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Enter Output Path (Mandatory):",
      "name": "Output Path",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "Sample Data Limit": {
     "currentValue": "5",
     "nuid": "89202490-b910-43a0-9673-c4136991178f",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "5",
      "label": "Sample Data Limit (Mandatory):",
      "name": "Sample Data Limit",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "5",
      "label": "Sample Data Limit (Mandatory):",
      "name": "Sample Data Limit",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "Schema": {
     "currentValue": "demodb",
     "nuid": "77e0720b-a517-4eec-ac80-09cfe3562767",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Enter Schema Name (Optional):",
      "name": "Schema",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Enter Schema Name (Optional):",
      "name": "Schema",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "Table": {
     "currentValue": "nyctrip",
     "nuid": "9fc3c05f-d8d4-4e4e-8e1a-95268e66c373",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Enter Table Name (Optional):",
      "name": "Table",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Enter Table Name (Optional):",
      "name": "Table",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
